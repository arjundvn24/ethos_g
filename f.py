# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Oq-7rHYAwcg2_pwcLAVH4kLZ2I261UQx
"""

import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# import transformers
plt.style.use('ggplot')

import nltk
nltk.download('vader_lexicon')
from nltk.sentiment import SentimentIntensityAnalyzer
from tqdm.notebook import tqdm
import re
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('omw-1.4')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
from nltk.stem import WordNetLemmatizer
from transformers import pipeline

sia = SentimentIntensityAnalyzer()
df = pd.read_json(r'temp.json')
df.to_csv(r'json_output.csv',index=None)
df = pd.read_csv('json_output.csv')
person = df['keyword'][0]
df = df[['title','text','date','author']]
df=df[df.date.notnull()]
df.reset_index(inplace=True)
df.reset_index(inplace=True)
df=df.drop(['index'],axis=1)
df=df.rename(columns={'level_0':'index'})

for i in range(df.shape[0]):
  var =df['date'][i]
  var=var[0:10]
  df['date'][i]=var

res = {}
for i, row in tqdm(df.iterrows(), total=len(df)):
    text = row['text']
    myid = row['index']
    res[myid] = sia.polarity_scores(text)

vaders = pd.DataFrame(res).T
vaders = vaders.reset_index()
vaders = vaders.merge(df, how='left')

label=[0]*vaders.shape[0]
sum=0
for i in range(vaders.shape[0]):
  sum+=vaders['compound'][i]
  if(vaders['compound'][i]>0):
    label[i]="Positive"
  else:
    label[i]="Negative"

vaders['label']=label
df2=vaders.copy()

summary=[0]*df2.shape[0]
for j in range(df2.shape[0]):
  ps = PorterStemmer()
  text=df2['text'][j]
  lemmatizer = WordNetLemmatizer()
  sentences = nltk.sent_tokenize(text)
  key = nltk.word_tokenize(person)
  corpus = []
  key
  for i in range(len(sentences)):
      review = re.sub('[^a-zA-Z]', ' ', sentences[i])
      review = review.lower()
      review = review.split()
      # review = [lemmatizer.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]
      review = ' '.join(review)
      for k in key:
        if(k in review):
          corpus.append(review)

  corpus=' '.join(corpus)
  corpus=corpus[len(corpus)-1024:len(corpus)-1]
  summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
  summary[j] = summarizer(corpus,max_length=30,min_length=10,do_sample=False)


lis=[]
for i in summary:
  for j in i:
    lis.append(j.get('summary_text'))
df2["summary"]=lis
df2= df2.loc[:,df2.columns!="text"]
df2.to_json('./search_page_al/src/file1.json', orient = 'records', compression = 'infer')
